{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from utils import *\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data as Data\n",
    "torch.use_deterministic_algorithms(True)\n",
    "from easydict import EasyDict as ed\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import skopt\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.plots import plot_convergence\n",
    "from captum.attr import FeaturePermutation\n",
    "if is_notebook():\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "clr = sns.color_palette(\"deep\")\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "split_by = 'material' # 'random' or 'material'\n",
    "\n",
    "validation = True\n",
    "physics_informed = False\n",
    "bayes_opt = True\n",
    "\n",
    "data_path = '../data/SNL_MSU_DOE_fatigue.xlsx'\n",
    "ckp_path = '../output/fatigue.pt'\n",
    "skopt_path = '../output/skopt.pt'\n",
    "\n",
    "n_calls = 200\n",
    "layers = [16, 64, 128, 256, 256, 128, 64, 16]\n",
    "\n",
    "# static_params = {'patience': 500, 'epoch': 2000, 'weight_decay': 0.0}\n",
    "# chosen_params = {'lr': 0.0062111007822585485, 'batch_size': 128}\n",
    "# SPACE = [\n",
    "#     Real(1e-4, 0.05, 'log-uniform', name='lr'),\n",
    "#     Categorical([32, 64, 128, 256, 512, 1024, 2048], name='batch_size'),\n",
    "# ]\n",
    "\n",
    "static_params = {'patience': 1000, 'epoch': 4000}\n",
    "chosen_params = {'lr': 0.0034560325081541875, 'weight_decay': 0.0019904362054363267, 'batch_size': 1024} # for random split\n",
    "# chosen_params = {'lr': 0.01, 'weight_decay': 0.005, 'batch_size': 1024}\n",
    "# chosen_params = {'lr': 0.0008576159573733293, 'weight_decay': 0.005, 'batch_size': 32}\n",
    "SPACE = [\n",
    "    Real(1e-3, 0.05, 'log-uniform', name='lr'),\n",
    "    Real(1e-5, 0.05, 'log-uniform', name='weight_decay'),\n",
    "    Categorical([32, 64, 128, 256, 512, 1024, 2048, 4096], name='batch_size'),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2948 911 949\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel(data_path, engine='openpyxl')\n",
    "\n",
    "name_mapping = {\n",
    "    'Material': 'Material',\n",
    "    'Resin Type': 'Resin Type',\n",
    "    'Vf, %': 'Fibre Volumn Fraction',\n",
    "    '%, 0 Deg': 'Percentage of Fibre in 0-deg Direction',\n",
    "    '%, 45 Deg': 'Percentage of Fibre in 45-deg Direction',\n",
    "    '%, 90 Deg': 'Percentage of Fibre in 90-deg Direction',\n",
    "    'other %': 'Percentage of Fibre in Other Direction',\n",
    "    'Thickness, mm': 'Thickness',\n",
    "    'Max. Stress, MPa': 'Maximum Stress',\n",
    "    'Min. Stress, MPa': 'Minimum Stress',\n",
    "    'R-value': 'Minimum/Maximum Stress',\n",
    "    'Freq., Hz': 'Frequency',\n",
    "    'E, GPa': 'Initial Elastic Modulus',\n",
    "    'Max. % Strain': 'Maximum Strain',\n",
    "    'Min. % Strain': 'Minimum Strain',\n",
    "    'Cycles': 'Cycles to Failure',\n",
    "    'Moisture Gain, %': 'Moisture Gain',\n",
    "    'Testing Temperature, OC': 'Temperature',\n",
    "    'Width, mm': 'Width',\n",
    "    'Static Max. Stress, MPa': 'Static Maximum Tensile Stress',\n",
    "    'Static Min. Stress, MPa': 'Static Maximum Compressive Stress',\n",
    "    'Static E, GPa': 'Static Elastic Modulus',\n",
    "    'Static Max. % Strain': 'Static Maximum Tensile Strain',\n",
    "    'Static Min. % Strain': 'Static Maximum Compressive Strain',\n",
    "    'Absolute Maximum Stress': 'Absolute Maximum Stress',\n",
    "    'Absolute Peak-to-peak Stress': 'Absolute Peak-to-peak Stress',\n",
    "    'Relative Maximum Stress': 'Relative Maximum Stress',\n",
    "    'Relative Peak-to-peak Stress': 'Relative Peak-to-peak Stress'\n",
    "}\n",
    "\n",
    "data = replace_column_name(data, name_mapping)\n",
    "\n",
    "feature_names = ['Percentage of Fibre in 0-deg Direction',\n",
    "                 'Percentage of Fibre in 45-deg Direction',\n",
    "                 'Percentage of Fibre in 90-deg Direction',\n",
    "                 'Percentage of Fibre in Other Direction',\n",
    "                 'Absolute Maximum Stress',\n",
    "                 'Absolute Peak-to-peak Stress',\n",
    "                 'Frequency',\n",
    "                 'Fibre Volumn Fraction',\n",
    "                 'Relative Maximum Stress',\n",
    "                 'Relative Peak-to-peak Stress',\n",
    "                 'Thickness',\n",
    "                 'Static Maximum Tensile Stress',\n",
    "                 'Static Maximum Tensile Strain',\n",
    "                 'Static Elastic Modulus']\n",
    "\n",
    "label_name = ['Cycles to Failure']\n",
    "\n",
    "tmp_data = data[feature_names+label_name+['Material', 'Lay-up']].copy().dropna(axis=0)\n",
    "\n",
    "material_names = tmp_data['Material'].copy()\n",
    "lay_up = tmp_data['Lay-up'].copy()\n",
    "mat_lay = np.array([x+y for x,y in zip(material_names, lay_up)], dtype=str)\n",
    "mat_lay_set = list(set(mat_lay))\n",
    "\n",
    "data = data[feature_names+label_name].dropna(axis=0)\n",
    "feature_data = data[feature_names]\n",
    "label_data = np.log10(data[label_name])\n",
    "\n",
    "X = torch.tensor(feature_data.values, dtype=torch.float32).to(device)\n",
    "y = torch.tensor(label_data.values, dtype=torch.float32).to(device)\n",
    "dataset = Data.TensorDataset(X, y)\n",
    "\n",
    "if validation:\n",
    "    train_val_test = np.array([0.6, 0.2, 0.2])\n",
    "    if split_by == 'random':\n",
    "        train_size = np.floor(len(label_data) * train_val_test[0]).astype(int)\n",
    "        val_size = np.floor(len(label_data) * train_val_test[1]).astype(int)\n",
    "        test_size = len(label_data) - train_size - val_size\n",
    "        train_dataset, val_dataset, test_dataset = Data.random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(0))\n",
    "    elif split_by == 'material':\n",
    "        train_dataset, val_dataset, test_dataset = split_by_material(dataset, mat_lay, mat_lay_set, train_val_test, validation)\n",
    "    else:\n",
    "        raise Exception('Split type not implemented')\n",
    "\n",
    "    print('Dataset size:', len(train_dataset), len(val_dataset), len(test_dataset))\n",
    "else:\n",
    "    train_test = np.array([0.8, 0.2])\n",
    "    if split_by == 'random':\n",
    "        train_size = np.floor(len(label_data) * train_test[0]).astype(int)\n",
    "        test_size = len(label_data) - train_size\n",
    "        train_dataset, test_dataset = Data.random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(0))\n",
    "    elif split_by == 'material':\n",
    "        train_dataset, test_dataset = split_by_material(dataset, mat_lay, mat_lay_set, train_test, validation)\n",
    "    else:\n",
    "        raise Exception('Split type not implemented')\n",
    "    print('Dataset size:', len(train_dataset), len(test_dataset))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "scaler.fit(train_dataset.dataset.tensors[0].cpu().numpy()[train_dataset.indices, :])\n",
    "# torch.data.Dataset.Subset share the same memory, so only transform once.\n",
    "transformed = scaler.transform(train_dataset.dataset.tensors[0].cpu().numpy())\n",
    "train_dataset.dataset.tensors = (torch.tensor(transformed, dtype=torch.float32).to(device), train_dataset.dataset.tensors[1])\n",
    "X = torch.tensor(scaler.transform(X.cpu().numpy()), dtype=torch.float32).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "test = mat_lay[test_dataset.indices]\n",
    "val = mat_lay[val_dataset.indices]\n",
    "train = mat_lay[train_dataset.indices]\n",
    "\n",
    "for name in val:\n",
    "    if name in train:\n",
    "        print('shit')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[<AxesSubplot:xlabel='lr', ylabel='Partial dependence'>,\n        <AxesSubplot:>, <AxesSubplot:>],\n       [<AxesSubplot:ylabel='weight_decay'>,\n        <AxesSubplot:xlabel='weight_decay', ylabel='Partial dependence'>,\n        <AxesSubplot:>],\n       [<AxesSubplot:xlabel='lr', ylabel='batch_size'>,\n        <AxesSubplot:xlabel='weight_decay'>,\n        <AxesSubplot:xlabel='batch_size', ylabel='Partial dependence'>]],\n      dtype=object)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.collections.PathCollection at 0x7fbd77b3ec88>,\n <matplotlib.collections.PathCollection at 0x7fbd77b3ef98>,\n <matplotlib.collections.PathCollection at 0x7fbd770be2b0>,\n <matplotlib.collections.PathCollection at 0x7fbd770be588>,\n <matplotlib.collections.PathCollection at 0x7fbd770be860>,\n <matplotlib.collections.PathCollection at 0x7fbd770beb38>,\n <matplotlib.collections.PathCollection at 0x7fbd770beda0>,\n <matplotlib.collections.PathCollection at 0x7fbd770c8048>,\n <matplotlib.collections.PathCollection at 0x7fbd770c82b0>,\n <matplotlib.collections.PathCollection at 0x7fbd770c8550>,\n <matplotlib.collections.PathCollection at 0x7fbd770c87f0>,\n <matplotlib.collections.PathCollection at 0x7fbd77b3ea58>,\n <matplotlib.collections.PathCollection at 0x7fbd770c8c50>,\n <matplotlib.spines.Spine at 0x7fbc9e994400>,\n <matplotlib.spines.Spine at 0x7fbc9e988fd0>,\n <matplotlib.spines.Spine at 0x7fbc9e988ef0>,\n <matplotlib.spines.Spine at 0x7fbc9e988b70>,\n <matplotlib.axis.XAxis at 0x7fbc9e994518>,\n <matplotlib.axis.YAxis at 0x7fbc9e988278>,\n Text(0.5, 1.0, ''),\n Text(0.0, 1.0, ''),\n Text(1.0, 1.0, ''),\n <matplotlib.patches.Rectangle at 0x7fbc9e9c40f0>]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax[1,0].get_children()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}